{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2ec5a-34d5-41d9-8d12-379745374d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactive mode. Enter user ids from your dataset to check recommendations. Type 'q' to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User id to inspect (or 'q' to quit):  348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar users to 348 (top 10):\n",
      "  user   2304  sim=0.563\n",
      "  user   3601  sim=0.559\n",
      "  user   4962  sim=0.518\n",
      "  user   6021  sim=0.514\n",
      "  user   4470  sim=0.503\n",
      "  user    713  sim=0.503\n",
      "  user    130  sim=0.499\n",
      "  user   4058  sim=0.496\n",
      "  user   2105  sim=0.494\n",
      "  user   2172  sim=0.491\n",
      "\n",
      "Top-10 for user 348:\n",
      " item_id                                            title    score\n",
      "    1209              Once Upon a Time in the West (1969) 5.720133\n",
      "     117            Young Poisoner's Handbook, The (1995) 5.720133\n",
      "     735       Cemetery Man (Dellamorte Dellamore) (1994) 5.720133\n",
      "    1449                       Waiting for Guffman (1996) 5.557563\n",
      "    2186                      Strangers on a Train (1951) 5.548595\n",
      "    1086                         Dial M for Murder (1954) 5.548595\n",
      "    1059    William Shakespeare's Romeo and Juliet (1996) 5.546474\n",
      "    3612                 Slipper and the Rose, The (1976) 5.333719\n",
      "    3083 All About My Mother (Todo Sobre Mi Madre) (1999) 5.324974\n",
      "    3339                             Cross of Iron (1977) 5.324974\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter group user ids (space/comma separated) to run group rounds, or press Enter to skip:  234,456,745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequential group recs for group [348, 234, 456, 745] (rounds=3)\n",
      "Round 1 top-10:\n",
      " item_id    score  disagreement                              title\n",
      "      84 6.306904      0.000000 Last Summer in the Hamptons (1995)\n",
      "     751 6.306904      0.000000                     Careful (1992)\n",
      "    2864 6.306904      0.000000                    Splendor (1999)\n",
      "    3349 6.306904      0.000000      Perils of Pauline, The (1947)\n",
      "    1812 6.009374      0.000000                  Wide Awake (1998)\n",
      "     831 5.916405      0.000000                   Stonewall (1995)\n",
      "    1421 5.780933      0.000000               Grateful Dead (1995)\n",
      "    1151 5.441087      0.000000                       Faust (1994)\n",
      "    2774 5.411851      0.977698       Better Than Chocolate (1999)\n",
      "    2675 5.389057      0.000000      Twice Upon a Yesterday (1998)\n",
      "Saved → outputs/round1_topn_group_348-234-456-745.csv\n",
      "\n",
      "Round 2 top-10:\n",
      " item_id    score  disagreement                                                                           title\n",
      "    2834 5.389057           0.0                                                 Very Thought of You, The (1998)\n",
      "     363 5.369466           0.0 Wonderful, Horrible Life of Leni Riefenstahl, The (Die Macht der Bilder) (1993)\n",
      "     787 5.369466           0.0                                              Gate of Heavenly Peace, The (1995)\n",
      "    3903 5.369466           0.0                                                                  Urbania (2000)\n",
      "    1807 5.243141           0.0                                                        Cool Dry Place, A (1998)\n",
      "    3817 5.195648           0.0                                Other Side of Sunday, The (Søndagsengler) (1996)\n",
      "    2905 5.175722           0.0                                                                  Sanjuro (1962)\n",
      "    1725 5.166460           0.0                                            Education of Little Tree, The (1997)\n",
      "     242 5.091737           0.0                                                   Farinelli: il castrato (1994)\n",
      "     892 5.073917           0.0                                                            Twelfth Night (1996)\n",
      "Saved → outputs/round2_topn_group_348-234-456-745.csv\n",
      "\n",
      "Round 3 top-10:\n",
      " item_id    score  disagreement                                    title\n",
      "     602 4.988649      0.000000            Great Day in Harlem, A (1994)\n",
      "     961 4.988649      0.000000            Little Lord Fauntleroy (1936)\n",
      "    3224 4.964307      0.367318 Woman in the Dunes (Suna no onna) (1964)\n",
      "     568 4.956085      0.000000                Bhaji on the Beach (1993)\n",
      "    2255 4.956085      0.000000             Young Doctors in Love (1982)\n",
      "     846 4.924372      0.097458                             Flirt (1995)\n",
      "     411 4.905749      0.000000                      You So Crazy (1994)\n",
      "    1397 4.905749      0.000000           Bastard Out of Carolina (1996)\n",
      "    3470 4.885239      0.313761                       Dersu Uzala (1974)\n",
      "    1310 4.871951      0.000000                             Hype! (1996)\n",
      "Saved → outputs/round3_topn_group_348-234-456-745.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# file: recsys_project.py\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data loading (1M → 100K → synthetic)\n",
    "# =========================\n",
    "def load_movielens_1m_if_available(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame] | Tuple[None, None]:\n",
    "    \"\"\"\n",
    "    Load MovieLens 1M if ratings.dat/movies.dat are present (UserID::MovieID::Rating::Timestamp).\n",
    "    Returns (ratings_df, items_df) or (None, None) if files missing.\n",
    "    \"\"\"\n",
    "    rpath = os.path.join(data_dir, \"ratings.dat\")\n",
    "    mpath = os.path.join(data_dir, \"movies.dat\")\n",
    "    if os.path.isfile(rpath) and os.path.isfile(mpath):\n",
    "        ratings = pd.read_csv(\n",
    "            rpath, sep=\"::\", engine=\"python\",\n",
    "            names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"], header=None,\n",
    "        )\n",
    "        items = pd.read_csv(\n",
    "            mpath, sep=\"::\", engine=\"python\",\n",
    "            names=[\"item_id\", \"title\", \"genres\"], header=None, encoding=\"latin-1\",\n",
    "        )[[\"item_id\", \"title\"]]\n",
    "        return ratings, items\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_movielens_100k_if_available(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame] | Tuple[None, None]:\n",
    "    \"\"\"\n",
    "    Load MovieLens 100K if u.data/u.item exist (tab/pipe separated).\n",
    "    \"\"\"\n",
    "    udata = os.path.join(data_dir, \"u.data\")\n",
    "    uitem = os.path.join(data_dir, \"u.item\")\n",
    "    if os.path.isfile(udata) and os.path.isfile(uitem):\n",
    "        ratings = pd.read_csv(\n",
    "            udata, sep=\"\\t\",\n",
    "            names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"], engine=\"python\",\n",
    "        )\n",
    "        items = pd.read_csv(\n",
    "            uitem, sep=\"|\", encoding=\"latin-1\",\n",
    "            header=None, engine=\"python\",\n",
    "        ).rename(columns={0: \"item_id\", 1: \"title\"})[[\"item_id\", \"title\"]]\n",
    "        return ratings, items\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def make_synthetic_dataset(n_users: int = 30, n_items: int = 60, seed: int = 7) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Small synthetic fallback so the pipeline runs anywhere.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    rows = []\n",
    "    user_latent = {u + 1: rng.uniform(-1, 1) for u in range(n_users)}\n",
    "    item_latent = {i + 1: rng.uniform(-1, 1) for i in range(n_items)}\n",
    "    for u in range(1, n_users + 1):\n",
    "        rated = rng.sample(range(1, n_items + 1), k=max(5, int(0.4 * n_items)))\n",
    "        for i in rated:\n",
    "            mu = 3.5 + 1.5 * (user_latent[u] * item_latent[i])\n",
    "            r = min(5, max(1, round(rng.gauss(mu, 0.8))))\n",
    "            rows.append((u, i, r, 0))\n",
    "    ratings = pd.DataFrame(rows, columns=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "    items = pd.DataFrame({\"item_id\": list(range(1, n_items + 1)),\n",
    "                          \"title\": [f\"Item {i}\" for i in range(1, n_items + 1)]})\n",
    "    return ratings, items\n",
    "\n",
    "\n",
    "def load_data(data_root: str) -> Tuple[pd.DataFrame, pd.DataFrame, bool]:\n",
    "    \"\"\"\n",
    "    Order: 1M → 100K → synthetic. Returns (ratings, items, is_synthetic).\n",
    "    \"\"\"\n",
    "    r1m, m1m = load_movielens_1m_if_available(data_root)\n",
    "    if r1m is not None:\n",
    "        return r1m, m1m, False\n",
    "    r100k, m100k = load_movielens_100k_if_available(data_root)\n",
    "    if r100k is not None:\n",
    "        return r100k, m100k, False\n",
    "    r, m = make_synthetic_dataset()\n",
    "    return r, m, True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# In-memory structures\n",
    "# =========================\n",
    "@dataclass\n",
    "class UIData:\n",
    "    by_user: Dict[int, Dict[int, float]]\n",
    "    by_item: Dict[int, Dict[int, float]]\n",
    "    user_means: Dict[int, float]\n",
    "    all_users: Set[int]\n",
    "    all_items: Set[int]\n",
    "\n",
    "\n",
    "def build_ui(ratings: pd.DataFrame) -> UIData:\n",
    "    \"\"\"\n",
    "    Build user→{item:rating}, item→{user:rating}, and user means.\n",
    "    \"\"\"\n",
    "    by_user: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "    by_item: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "    for row in ratings.itertuples(index=False):\n",
    "        u = int(row.user_id); i = int(row.item_id); r = float(row.rating)\n",
    "        by_user[u][i] = r\n",
    "        by_item[i][u] = r\n",
    "    user_means = {u: (sum(items.values()) / len(items)) if items else 0.0 for u, items in by_user.items()}\n",
    "    return UIData(by_user, by_item, user_means, set(by_user.keys()), set(by_item.keys()))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Similarity + Predictions\n",
    "# =========================\n",
    "def pearson_similarity(u_ratings: Dict[int, float], v_ratings: Dict[int, float],\n",
    "                       u_mean: float, v_mean: float, min_overlap: int = 2, shrinkage: float = 10.0) -> float:\n",
    "    \"\"\"\n",
    "    Centered Pearson with overlap-based shrinkage.\n",
    "    Returns 0 if overlap too small or variance ~0.\n",
    "    \"\"\"\n",
    "    common = set(u_ratings.keys()) & set(v_ratings.keys())\n",
    "    n = len(common)\n",
    "    if n < min_overlap:\n",
    "        return 0.0\n",
    "    num = den_u = den_v = 0.0\n",
    "    for i in common:\n",
    "        du = u_ratings[i] - u_mean\n",
    "        dv = v_ratings[i] - v_mean\n",
    "        num += du * dv\n",
    "        den_u += du * du\n",
    "        den_v += dv * dv\n",
    "    if den_u <= 1e-12 or den_v <= 1e-12:\n",
    "        return 0.0\n",
    "    rho = num / math.sqrt(den_u * den_v)\n",
    "    return (n / (n + shrinkage)) * rho  # why: stabilize small-overlap noise\n",
    "\n",
    "\n",
    "def top_k_neighbors(u: int, data: UIData, k: int = 25) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Top-k positive-similarity neighbors by Pearson.\n",
    "    \"\"\"\n",
    "    sims: List[Tuple[int, float]] = []\n",
    "    u_r = data.by_user.get(u, {})\n",
    "    for v in data.all_users:\n",
    "        if v == u:\n",
    "            continue\n",
    "        s = pearson_similarity(u_r, data.by_user[v], data.user_means.get(u, 0.0), data.user_means.get(v, 0.0))\n",
    "        if s > 0:\n",
    "            sims.append((v, s))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[:k]\n",
    "\n",
    "\n",
    "def predict_user_scores(u: int, data: UIData, k: int = 25) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Mean-centered kNN deviations; returns predictions for items the user hasn't rated.\n",
    "    \"\"\"\n",
    "    neighbors = top_k_neighbors(u, data, k=k)\n",
    "    seen = set(data.by_user.get(u, {}).keys())\n",
    "    candidate_items = data.all_items - seen\n",
    "    numer: Dict[int, float] = defaultdict(float)\n",
    "    denom: Dict[int, float] = defaultdict(float)\n",
    "    for v, sim in neighbors:\n",
    "        v_mean = data.user_means[v]\n",
    "        for i, rv in data.by_user[v].items():\n",
    "            if i in seen:\n",
    "                continue\n",
    "            numer[i] += sim * (rv - v_mean)\n",
    "            denom[i] += abs(sim)\n",
    "    u_mean = data.user_means.get(u, 0.0)\n",
    "    return {i: (u_mean + numer[i] / denom[i]) for i in candidate_items if denom[i] > 1e-12}\n",
    "\n",
    "\n",
    "def top_n_for_user(u: int, data: UIData, items: pd.DataFrame, n: int = 10, k: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a Top-N DataFrame: item_id, title, score.\n",
    "    \"\"\"\n",
    "    preds = predict_user_scores(u, data, k=k)\n",
    "    if not preds:\n",
    "        return pd.DataFrame(columns=[\"item_id\", \"title\", \"score\"])\n",
    "    df = pd.DataFrame([{\"item_id\": i, \"score\": s} for i, s in preds.items()])\n",
    "    return df.merge(items, on=\"item_id\", how=\"left\").sort_values(\"score\", ascending=False).head(n)[\n",
    "        [\"item_id\", \"title\", \"score\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Group + Sequential\n",
    "# =========================\n",
    "def score_items_for_group_dict(group: List[int], data: UIData, k: int = 25) -> Dict[int, Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    item_id → {user_id: predicted_score} for given group (missing preds omitted).\n",
    "    \"\"\"\n",
    "    per_user_preds = {u: predict_user_scores(u, data, k=k) for u in group}\n",
    "    all_items = set().union(*[set(p.keys()) for p in per_user_preds.values()])\n",
    "    res: Dict[int, Dict[int, float]] = {}\n",
    "    for i in all_items:\n",
    "        res[i] = {u: per_user_preds[u][i] for u in group if i in per_user_preds[u]}\n",
    "    return res\n",
    "\n",
    "\n",
    "def rank_group_weighted(scores: Dict[int, Dict[int, float]], items: pd.DataFrame,\n",
    "                        group: List[int], member_weights: List[float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Weighted average over available members only; renormalize weights over present members.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, u2s in scores.items():\n",
    "        num = den = 0.0\n",
    "        for w, u in zip(member_weights, group):\n",
    "            if u in u2s:\n",
    "                num += w * u2s[u]\n",
    "                den += w\n",
    "        if den > 0:\n",
    "            mu = num / den\n",
    "            std = float(np.std(list(u2s.values()))) if len(u2s) > 1 else 0.0\n",
    "            rows.append((i, mu, std))\n",
    "    df = pd.DataFrame(rows, columns=[\"item_id\", \"score\", \"disagreement\"]).merge(items, on=\"item_id\", how=\"left\")\n",
    "    return df.sort_values([\"score\", \"disagreement\"], ascending=[False, True])\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert coverage to weights that favor lower-coverage users.\n",
    "    \"\"\"\n",
    "    z = (x - x.min()) / max(1e-12, (x.max() - x.min()))\n",
    "    z = -z / max(1e-6, temperature)  # why: invert to upweight under-served members\n",
    "    e = np.exp(z - np.max(z))\n",
    "    return e / np.sum(e)\n",
    "\n",
    "\n",
    "def sequential_group_recs(group: List[int], data: UIData, items: pd.DataFrame, rounds: int = 3,\n",
    "                          k: int = 25, alpha: float = 1.0, topn: int = 10) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Multi-round group recs with adaptive fairness via coverage-based weights.\n",
    "    \"\"\"\n",
    "    coverage = {u: 0.0 for u in group}\n",
    "    already: Set[int] = set()\n",
    "    outputs: List[pd.DataFrame] = []\n",
    "    for _ in range(rounds):\n",
    "        full_scores = score_items_for_group_dict(group, data, k=k)\n",
    "        if already:\n",
    "            full_scores = {i: u2s for i, u2s in full_scores.items() if i not in already}\n",
    "        cov_vec = np.array([coverage[u] for u in group], dtype=float)\n",
    "        weights = softmax(alpha * cov_vec)\n",
    "        df_weighted = rank_group_weighted(full_scores, items, group, member_weights=list(weights))\n",
    "        top_df = df_weighted.head(topn).reset_index(drop=True)\n",
    "        outputs.append(top_df)\n",
    "        for _, row in top_df.iterrows():\n",
    "            i = int(row[\"item_id\"])\n",
    "            already.add(i)\n",
    "            for u in group:\n",
    "                if u in full_scores[i]:\n",
    "                    coverage[u] += full_scores[i][u]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# =========================\n",
    "# IO helpers\n",
    "# =========================\n",
    "def ensure_outputs_dir(path: str = \"outputs\") -> str:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: str):\n",
    "    df.to_csv(path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Interactive UX\n",
    "# =========================\n",
    "def prompt_int(prompt: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Read an int from input; return None on empty/q.\n",
    "    \"\"\"\n",
    "    s = input(prompt).strip()\n",
    "    if s == \"\" or s.lower() == \"q\":\n",
    "        return None\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid integer (or 'q' to quit).\")\n",
    "        return prompt_int(prompt)\n",
    "\n",
    "\n",
    "def prompt_int_list(prompt: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Parse comma/space-separated ints; empty → [].\n",
    "    \"\"\"\n",
    "    s = input(prompt).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    parts = [p for chunk in s.split(\",\") for p in chunk.split()]\n",
    "    out: List[int] = []\n",
    "    for p in parts:\n",
    "        if p.lower() == \"q\":\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except ValueError:\n",
    "            print(f\"Ignored non-integer token: {p}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def interactive_session(ui: UIData, items: pd.DataFrame, k: int, topn: int, rounds: int, alpha: float):\n",
    "    \"\"\"\n",
    "    Loop: ask for a user id → show personal recs; optionally ask for a group → sequential recs.\n",
    "    \"\"\"\n",
    "    print(\"\\nInteractive mode. Enter user ids from your dataset to check recommendations. Type 'q' to exit.\\n\")\n",
    "    while True:\n",
    "        u = prompt_int(\"User id to inspect (or 'q' to quit): \")\n",
    "        if u is None:\n",
    "            break\n",
    "        if u not in ui.all_users:\n",
    "            print(\"User not found in data. Try another id.\")\n",
    "            continue\n",
    "\n",
    "        # Individual\n",
    "        neigh = top_k_neighbors(u, ui, k=k)\n",
    "        print(f\"\\nMost similar users to {u} (top {min(10, len(neigh))}):\")\n",
    "        for v, s in neigh[:10]:\n",
    "            print(f\"  user {v:>6}  sim={s:.3f}\")\n",
    "        df_top = top_n_for_user(u, ui, items, n=topn, k=k)\n",
    "        print(f\"\\nTop-{topn} for user {u}:\\n{df_top.to_string(index=False)}\\n\")\n",
    "\n",
    "        # Group (optional)\n",
    "        group = prompt_int_list(\"Enter group user ids (space/comma separated) to run group rounds, or press Enter to skip: \")\n",
    "        group = [g for g in group if g in ui.all_users and g != u]\n",
    "        if group:\n",
    "            # why: include the inspected user by default to make a realistic group\n",
    "            if u not in group:\n",
    "                group = [u] + group\n",
    "            group = list(dict.fromkeys(group))  # keep order, drop dups\n",
    "            print(f\"\\nSequential group recs for group {group} (rounds={rounds})\")\n",
    "            seq = sequential_group_recs(group, ui, items, rounds=rounds, k=k, alpha=alpha, topn=topn)\n",
    "            out_dir = ensure_outputs_dir()\n",
    "            for r, df in enumerate(seq, start=1):\n",
    "                path = os.path.join(out_dir, f\"round{r}_topn_group_{'-'.join(map(str, group))}.csv\")\n",
    "                save_df(df, path)\n",
    "                print(f\"Round {r} top-{topn}:\\n{df.head(10).to_string(index=False)}\\nSaved → {path}\\n\")\n",
    "\n",
    "        cont = input(\"Check another user? (y/n): \").strip().lower()\n",
    "        if cont not in (\"y\", \"yes\"):\n",
    "            break\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Entry point\n",
    "# =========================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Movie Recommender: interactive user & group recommendations\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"data\",\n",
    "                        help=\"Directory with MovieLens files (1M: ratings.dat/movies.dat or 100K: u.data/u.item).\")\n",
    "    parser.add_argument(\"--k\", type=int, default=25, help=\"k nearest neighbors.\")\n",
    "    parser.add_argument(\"--topn\", type=int, default=10, help=\"Top-N recommendations to display.\")\n",
    "    parser.add_argument(\"--rounds\", type=int, default=3, help=\"Sequential rounds for group recommendations.\")\n",
    "    parser.add_argument(\"--alpha_seq\", type=float, default=1.0, help=\"Fairness strength across rounds.\")\n",
    "    args, _ = parser.parse_known_args()  # why: ignore Jupyter's extra -f arg\n",
    "\n",
    "    ratings, items, is_synth = load_data(args.data_dir)\n",
    "    if is_synth:\n",
    "        print(\"MovieLens files not found. Using synthetic data so the pipeline runs end-to-end.\")\n",
    "    ui = build_ui(ratings)\n",
    "\n",
    "    interactive_session(ui, items, k=args.k, topn=args.topn, rounds=args.rounds, alpha=args.alpha_seq)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7c2f8-e7df-4d82-b921-72c707e1a000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
